function findPolicyForLarge(data)
    numStates=312020;
    numActions=9;
    uniformPolicy=ones(numStates,1);

    # Manipulating data
    s=data[1];
    a=data[2];
    r=data[3];
    sp=data[4];
    numRows=size(data,1)
    uniqueInitialStates=unique(s);
    uniqueFinalStates=unique(sp);
    # We've found that these are the same 500 states in both cases
    numUniqueStates=size(uniqueInitialStates, 1);
    s_new_numbering=zeros(Int64,numRows);
    sp_new_numbering=zeros(Int64,numRows);
    for i=1:numRows
        sp_new_numbering[i] = findfirst(fs -> fs==sp[i], uniqueFinalStates);
        s_new_numbering[i] = findfirst(is -> is==s[i], uniqueInitialStates);
    end

    # Q-learning
    discount_Q=0.95; # given to us!
    alpha=1/numUniqueStates;
    @show numUniqueStates
    Qinit=ones(numUniqueStates,numActions);
    for si=1:numUniqueStates
        realStateIndex=uniqueInitialStates[si];
        if (realStateIndex==150413 || realStateIndex==151203 || realStateIndex==150211)
            if (realStateIndex == 150211)
                break;
            end
            for ai=1:9
                if (ai==1)
                    Qinit[si,ai]=10000000;
                else
                    Qinit[si,ai]=0;
                end
            end
        else
            for ai=1:9
                if (ai>=5)
                    Qinit[si,ai]=100;
                else
                    Qinit[si,ai]=0;
                end
            end
        end
    end
    numIterValues=[1 5 20 50 10 300];
    optimalPolicy=zeros(Int64,numStates);
    @show Qinit
    for x=1:length(numIterValues)
        numIter=numIterValues[x];
        Qvalues=doQLearningOnUniqueStates(numUniqueStates,numActions,s_new_numbering,a,r,sp_new_numbering,discount_Q,alpha,numIter,Qinit=Qinit, threshold=0);
        optimalPolicyForUniqueStates=findOptimalPolicyFromQ(Qvalues,numUniqueStates);
        write_policy(optimalPolicyForUniqueStates,"largeQOnlyUniqueStates"*string(numIter)*"Iter.policy");
        # convert to full optimal policy
        optimalPolicy=constructFullOptimalPolicyFromSubset(optimalPolicyForUniqueStates, numStates, uniqueInitialStates)
        write_policy(optimalPolicy,"largeQ"*string(numIter)*"Iter.policy");
    end
    return optimalPolicy;

    # Value iteration by approximating T and R

    # @show numRows

    # @show sp_new_numbering[1:10]
    # T=zeros(numUniqueStates,numActions,numUniqueStates);
    # R=zeros(numUniqueStates,numActions);
    # # Can write R(s,a) which is mostly zeros except at a few states
    # G0Indices=findall(ri -> ri>0, r);
    # @show G0Indices[1:10]
    # G0Rewards=r[G0Indices];
    # G0Actions=a[G0Indices];
    # G0States=s_new_numbering[G0Indices];
    # # @show a[1:10]
    # # @show G0States
    # for k=1:length(G0Indices)
    #     R[G0States[k],G0Actions[k]]=G0Rewards[k];
    # end
    # # T(sp|s,a) can be approximated
    # # for a=1-4, T(s|s,a)=1 and T(s'|s,a) where s' not equal to s is 0
    # for a=1:4
    #     for s=1:numUniqueStates
    #         T[s,a,s]=1;
    #     end
    # end
    # # for a=5-9 T(s'|s,a) is evenly distributed
    # # Can improve on this because for some sk T(s'|sk,a) is only 1 if s'=sk
    # # But we would have to identify those sk
    # for a=5:9
    #     for s=1:numUniqueStates
    #         for sp=1:numUniqueStates
    #             T[s,a,sp]=1/numUniqueStates;
    #         end
    #     end
    # end
    # # @show T
    # # With these R and T estimates, try value iteration
    # discount=0.95;
    # delta=0.1;
    # maxIter=1000;
    # optimalPolicyForUniqueStates=doValueIteration(T,R,numUniqueStates,numActions, discount, delta, maxIter);
    # # Need to convert to an optimal policy for all numStates
    # optimalPolicy=zeros(numStates,1);
    # for i=1:numStates
    #     if (i in uniqueInitialStates)
    #         idxInUniqueStates = findfirst(is -> is==i, uniqueInitialStates);
    #         optimalPolicy[i]=optimalPolicyForUniqueStates[idxInUniqueStates];
    #     else
    #         # Pick an action that is not stationary, i.e. not 1-4
    #         optimalPolicy[i]=5;
    #     end
    # end
    # return optimalPolicy

end


function doQLearningOnUniqueStates(numStates,numActions,svals,avals,rvals,spvals,discount,alpha,maxNumIter; Qinit=zeros(numStates,numActions), threshold=0.01)
    Q=Qinit;
    for k=1:maxNumIter
        for i in 1:size(svals, 1)
            s=svals[i];
            a=avals[i];
            r=rvals[i];
            sp=spvals[i];
            (maxValue,ind_max)=findmax(Q[sp,:]);
            Q[s,a] = Q[s,a] + alpha*(r + discount*maxValue - Q[s,a])
        end
    end
    return Q;
    # Q=Qinit;
    # numIter=1;
    # update=threshold+1; # some non-zero initialization
    # while (numIter<=maxNumIter && update > threshold)
    #     update=0;
    #     for i in 1:size(svals, 1)
    #         s=svals[i];
    #         a=avals[i];
    #         r=rvals[i];
    #         sp=spvals[i];
    #         (maxValue,ind_max)=findmax(Q[sp,:]);
    #         q=copy(Q[s,a]);
    #         Q[s,a] = q + alpha*(r + discount*maxValue - q);
    #         # Find the biggest change in Q values for this episode
    #         update=max(update, alpha*(r + discount*maxValue - q));
    #     end
    #     @show(update)
    #     numIter+=1;
    # end
    # @show(numIter)
    # @show(update)
    # return Q;
end



function doQLearningOld(numStates,numActions,data,discount,alpha,numIter; Qinit=zeros(numStates,numActions))
    Q=Qinit;
    for k=1:numIter
        for i in 1:size(data, 1)
            s=data[i,1];
            a=data[i,2];
            r=data[i,3];
            sp=data[i,4];
            (maxValue,ind_max)=findmax(Q[sp,:]);
            Q[s,a] = Q[s,a] + alpha*(r + discount*maxValue - Q[s,a])
        end
    end
    return Q;
end